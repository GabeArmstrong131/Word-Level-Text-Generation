{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hCKrPvslHf2J"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8ZMLShLIDUr"
      },
      "source": [
        "## **Description**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dppjB4T1IIBn"
      },
      "source": [
        "**Word-level text generation in python using keras model and tensorflow.**\n",
        "\n",
        "Upload a text document that is then cleaned, prepared, and turned into sequences of 51 words (50 input and 1 output).\n",
        "\n",
        "Words and sequences are tokenized using the keras tokenizer, mapping unique words to integers and encoding input sequences.\n",
        "\n",
        "Sequences are then fit to a sequential language model. Accuracy is relative to the number of epochs, the size of the embedding vector space, and the number of neurons.\n",
        "\n",
        "A randomized seed text is generated which is encoded using the same tokenizer that was used for input sequences. Each word is predicted as an integer using np.argmax(model.predict()) which is then mapped to the word from the tokenizer. The text is then fully generated.\n",
        "\n",
        "With varying levels of accuracy in the model, some text generation is more realistic than others. Even with high accuracy there is a chance for the text to be nonsensical depending on the seed text and the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHybL84ZXKES"
      },
      "source": [
        "## **Upload File**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83JjeDESXPN4"
      },
      "source": [
        " #upload new file if necessary\n",
        " #from google.colab import files\n",
        " #uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYSqBNxfXJ_q"
      },
      "source": [
        "## **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v--nT7JXP1L"
      },
      "source": [
        "import string\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from random import randint\n",
        "from pickle import load\n",
        "from pickle import dump\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07zm5oK5Us2H"
      },
      "source": [
        "## **Load Text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgIXu0u9VFwR"
      },
      "source": [
        "def load_text(fileName):\n",
        "  file = open(fileName, 'r')\n",
        "  text = file.read()\n",
        "  file.close\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Pe15egOV_H8",
        "outputId": "339265b0-1515-42f6-8664-a62a90f1bb4e"
      },
      "source": [
        "textFile = 'gatsby.txt'\n",
        "text = load_text(textFile)\n",
        "# checks load\n",
        "print(text[:200])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿\n",
            "                                  I\n",
            "\n",
            "In my younger and more vulnerable years my father gave me some advice\n",
            "that I’ve been turning over in my mind ever since.\n",
            "\n",
            "“Whenever you feel like criticizing any\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DbDMoFSVqIY"
      },
      "source": [
        "## **Prepare Text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9UxOXLaV0wb",
        "outputId": "ecd4552d-22c0-4f93-a5eb-a85d29e02151"
      },
      "source": [
        "# cleans up characters that could cause issues\n",
        "def text_prep(text):\n",
        " text = text.replace('--', ' ')\n",
        " words = text.split()\n",
        " table = str.maketrans('', '', string.punctuation)\n",
        " words = [w.translate(table) for w in words]\n",
        " words = [word for word in words if word.isalpha()]\n",
        " words = [word.lower() for word in words]\n",
        " return words\n",
        "\n",
        "tokens = text_prep(text)\n",
        "\n",
        "# quick functionality checks\n",
        "print(tokens[:100])\n",
        "print('Total words: %d' % len(tokens))\n",
        "print('Unique words: %d' % len(set(tokens)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'in', 'my', 'younger', 'and', 'more', 'vulnerable', 'years', 'my', 'father', 'gave', 'me', 'some', 'advice', 'that', 'been', 'turning', 'over', 'in', 'my', 'mind', 'ever', 'since', 'you', 'feel', 'like', 'criticizing', 'he', 'told', 'me', 'remember', 'that', 'all', 'the', 'people', 'in', 'this', 'world', 'had', 'the', 'advantages', 'that', 'he', 'say', 'any', 'more', 'but', 'always', 'been', 'unusually', 'communicative', 'in', 'a', 'reserved', 'way', 'and', 'i', 'understood', 'that', 'he', 'meant', 'a', 'great', 'deal', 'more', 'than', 'that', 'in', 'consequence', 'inclined', 'to', 'reserve', 'all', 'judgements', 'a', 'habit', 'that', 'has', 'opened', 'up', 'many', 'curious', 'natures', 'to', 'me', 'and', 'also', 'made', 'me', 'the', 'victim', 'of', 'not', 'a', 'few', 'veteran', 'bores', 'the', 'abnormal', 'mind']\n",
            "Total words: 46724\n",
            "Unique words: 5987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51oved9PTp8Q",
        "outputId": "a8968e08-b338-4b1c-a707-17018627f7f2"
      },
      "source": [
        "length = 50 + 1\n",
        "sequences = list()\n",
        "for i in range(length, len(tokens)):\n",
        "  short = tokens[i-length:i]\n",
        "  line = ' '.join(short)\n",
        "  sequences.append(line)\n",
        "\n",
        "# save tokens to file\n",
        "def save_doc(lines, filename):\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()\n",
        " \n",
        "# save sequences to file\n",
        "out_filename = 'text_sequences.txt'\n",
        "save_doc(sequences, out_filename)\n",
        "\n",
        "# check sequences\n",
        "print('All Sequences: %d' % len(sequences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All Sequences: 46673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHjMuUj7GxCF"
      },
      "source": [
        "## **Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeiyKBvru6p8"
      },
      "source": [
        "# redefine load text for loading doc\n",
        "def load_doc(fileName):\n",
        "  file = open(fileName, 'r')\n",
        "  text = file.read()\n",
        "  file.close\n",
        "  return text\n",
        "\n",
        "in_filename = 'text_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')\n",
        "\n",
        "# encodes sequences to integers values\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# input and output\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgZXPUkQu1PF"
      },
      "source": [
        "## **Train Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvdv4oStzmOu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d69d3d74-e4db-48a9-d851-27f694752c6a"
      },
      "source": [
        "# define model\n",
        "# accuracy of generation at 100-200 epochs is functional\n",
        "# may take too long to run on higher values, additional accuracy not worth time\n",
        "# EMBEDDING: 50, 100, 300, etc.\n",
        "# LSTM: higher the better, 100 standard\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 150, input_length=seq_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "# summarize model\n",
        "print(model.summary())\n",
        "\n",
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "model.fit(X, y, batch_size=128, epochs=100)\n",
        "\n",
        "# save the model\n",
        "model.save('model.h5')\n",
        "# save the tokenizer\n",
        "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 50, 150)           898200    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 50, 100)           100400    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               80400     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5988)              604788    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,693,888\n",
            "Trainable params: 1,693,888\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "365/365 [==============================] - 19s 35ms/step - loss: 6.7591 - accuracy: 0.0517\n",
            "Epoch 2/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 6.3467 - accuracy: 0.0565\n",
            "Epoch 3/100\n",
            "365/365 [==============================] - 13s 36ms/step - loss: 6.1336 - accuracy: 0.0763\n",
            "Epoch 4/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 5.9358 - accuracy: 0.0948\n",
            "Epoch 5/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 5.7759 - accuracy: 0.1049\n",
            "Epoch 6/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 5.6506 - accuracy: 0.1090\n",
            "Epoch 7/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 5.5469 - accuracy: 0.1131\n",
            "Epoch 8/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 5.4505 - accuracy: 0.1181\n",
            "Epoch 9/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 5.3597 - accuracy: 0.1229\n",
            "Epoch 10/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 5.2668 - accuracy: 0.1288\n",
            "Epoch 11/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 5.1702 - accuracy: 0.1353\n",
            "Epoch 12/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 5.0748 - accuracy: 0.1392\n",
            "Epoch 13/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 4.9781 - accuracy: 0.1447\n",
            "Epoch 14/100\n",
            "365/365 [==============================] - 13s 37ms/step - loss: 4.8824 - accuracy: 0.1506\n",
            "Epoch 15/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 4.7905 - accuracy: 0.1545\n",
            "Epoch 16/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 4.6991 - accuracy: 0.1606\n",
            "Epoch 17/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 4.6077 - accuracy: 0.1660\n",
            "Epoch 18/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 4.5199 - accuracy: 0.1731\n",
            "Epoch 19/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 4.4335 - accuracy: 0.1778\n",
            "Epoch 20/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 4.3476 - accuracy: 0.1819\n",
            "Epoch 21/100\n",
            "365/365 [==============================] - 13s 36ms/step - loss: 4.2616 - accuracy: 0.1871\n",
            "Epoch 22/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 4.1775 - accuracy: 0.1938\n",
            "Epoch 23/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 4.0936 - accuracy: 0.2010\n",
            "Epoch 24/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 4.0107 - accuracy: 0.2076\n",
            "Epoch 25/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 3.9343 - accuracy: 0.2162\n",
            "Epoch 26/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 3.8568 - accuracy: 0.2239\n",
            "Epoch 27/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 3.7831 - accuracy: 0.2320\n",
            "Epoch 28/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 3.7092 - accuracy: 0.2418\n",
            "Epoch 29/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 3.6358 - accuracy: 0.2536\n",
            "Epoch 30/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 3.5694 - accuracy: 0.2599\n",
            "Epoch 31/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 3.5063 - accuracy: 0.2697\n",
            "Epoch 32/100\n",
            "365/365 [==============================] - 12s 34ms/step - loss: 3.4421 - accuracy: 0.2781\n",
            "Epoch 33/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 3.3773 - accuracy: 0.2891\n",
            "Epoch 34/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 3.3163 - accuracy: 0.2968\n",
            "Epoch 35/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 3.2610 - accuracy: 0.3060\n",
            "Epoch 36/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 3.2044 - accuracy: 0.3134\n",
            "Epoch 37/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 3.1459 - accuracy: 0.3243\n",
            "Epoch 38/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 3.0912 - accuracy: 0.3321\n",
            "Epoch 39/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 3.0384 - accuracy: 0.3419\n",
            "Epoch 40/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 2.9891 - accuracy: 0.3504\n",
            "Epoch 41/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 2.9336 - accuracy: 0.3600\n",
            "Epoch 42/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 2.8842 - accuracy: 0.3673\n",
            "Epoch 43/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 2.8382 - accuracy: 0.3755\n",
            "Epoch 44/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 2.7912 - accuracy: 0.3836\n",
            "Epoch 45/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 2.7416 - accuracy: 0.3909\n",
            "Epoch 46/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 2.6991 - accuracy: 0.4005\n",
            "Epoch 47/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 2.6500 - accuracy: 0.4096\n",
            "Epoch 48/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 2.6133 - accuracy: 0.4153\n",
            "Epoch 49/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 2.5755 - accuracy: 0.4233\n",
            "Epoch 50/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 2.5311 - accuracy: 0.4300\n",
            "Epoch 51/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 2.4910 - accuracy: 0.4370\n",
            "Epoch 52/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 2.4534 - accuracy: 0.4442\n",
            "Epoch 53/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 2.4134 - accuracy: 0.4533\n",
            "Epoch 54/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 2.3774 - accuracy: 0.4575\n",
            "Epoch 55/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 2.3345 - accuracy: 0.4654\n",
            "Epoch 56/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 2.3023 - accuracy: 0.4746\n",
            "Epoch 57/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 2.2639 - accuracy: 0.4814\n",
            "Epoch 58/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 2.2368 - accuracy: 0.4843\n",
            "Epoch 59/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 2.1955 - accuracy: 0.4930\n",
            "Epoch 60/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 2.1672 - accuracy: 0.5010\n",
            "Epoch 61/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 2.1377 - accuracy: 0.5046\n",
            "Epoch 62/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 2.1007 - accuracy: 0.5123\n",
            "Epoch 63/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 2.0640 - accuracy: 0.5211\n",
            "Epoch 64/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 2.0398 - accuracy: 0.5261\n",
            "Epoch 65/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 2.0088 - accuracy: 0.5307\n",
            "Epoch 66/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.9750 - accuracy: 0.5369\n",
            "Epoch 67/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 1.9413 - accuracy: 0.5449\n",
            "Epoch 68/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.9153 - accuracy: 0.5495\n",
            "Epoch 69/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.8914 - accuracy: 0.5542\n",
            "Epoch 70/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.8630 - accuracy: 0.5619\n",
            "Epoch 71/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.8320 - accuracy: 0.5684\n",
            "Epoch 72/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.8070 - accuracy: 0.5724\n",
            "Epoch 73/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.7801 - accuracy: 0.5791\n",
            "Epoch 74/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.7534 - accuracy: 0.5853\n",
            "Epoch 75/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.7240 - accuracy: 0.5904\n",
            "Epoch 76/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.7045 - accuracy: 0.5954\n",
            "Epoch 77/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.6792 - accuracy: 0.5976\n",
            "Epoch 78/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.6517 - accuracy: 0.6079\n",
            "Epoch 79/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.6301 - accuracy: 0.6103\n",
            "Epoch 80/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.5997 - accuracy: 0.6177\n",
            "Epoch 81/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.5753 - accuracy: 0.6233\n",
            "Epoch 82/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.5546 - accuracy: 0.6269\n",
            "Epoch 83/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.5297 - accuracy: 0.6310\n",
            "Epoch 84/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.5057 - accuracy: 0.6367\n",
            "Epoch 85/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.4804 - accuracy: 0.6447\n",
            "Epoch 86/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.4572 - accuracy: 0.6490\n",
            "Epoch 87/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.4513 - accuracy: 0.6499\n",
            "Epoch 88/100\n",
            "365/365 [==============================] - 13s 34ms/step - loss: 1.4240 - accuracy: 0.6564\n",
            "Epoch 89/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.3975 - accuracy: 0.6631\n",
            "Epoch 90/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.3718 - accuracy: 0.6682\n",
            "Epoch 91/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.3515 - accuracy: 0.6723\n",
            "Epoch 92/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.3310 - accuracy: 0.6773\n",
            "Epoch 93/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.3150 - accuracy: 0.6791\n",
            "Epoch 94/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.2952 - accuracy: 0.6855\n",
            "Epoch 95/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.2780 - accuracy: 0.6883\n",
            "Epoch 96/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.2544 - accuracy: 0.6947\n",
            "Epoch 97/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.2309 - accuracy: 0.6997\n",
            "Epoch 98/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.2078 - accuracy: 0.7057\n",
            "Epoch 99/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.1960 - accuracy: 0.7074\n",
            "Epoch 100/100\n",
            "365/365 [==============================] - 13s 35ms/step - loss: 1.1858 - accuracy: 0.7076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgcKGHb59hZy"
      },
      "source": [
        "## **Generate Text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36PxBfg89rry"
      },
      "source": [
        "# load data from language model\n",
        "def load_doc(filename):\n",
        "\tfile = open(filename, 'r')\n",
        "\ttext = file.read()\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# load text sequences\n",
        "in_filename = 'text_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')\n",
        "\n",
        "# set sequence length\n",
        "seq_length = len(lines[0].split()) - 1\n",
        "\n",
        "# load saved model and tokenizer\n",
        "model = load_model('model.h5')\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5HIJFtJ99lB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20fd3992-0916-4d45-b427-38fcf896860b"
      },
      "source": [
        "# generate a sequence from a language model\n",
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "\tresult = list()\n",
        "\tinput_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "\tfor _ in range(n_words):\n",
        "\t\t# encode text as integer\n",
        "\t\tencoded = tokenizer.texts_to_sequences([input_text])[0]\n",
        "\t\t# truncate sequences\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\t\t# predict probabilities for each integer\n",
        "\t\tyhat = np.argmax(model.predict(encoded))\n",
        "\t\t# map predicted integer index to word\n",
        "\t\toutput = ''\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\toutput = word\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tinput_text += ' ' + output\n",
        "\t\tresult.append(output)\n",
        "\treturn ' '.join(result)\n",
        " \n",
        "# select and print seed text\n",
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(\"Seed text: \" + seed_text + '\\n')\n",
        "\n",
        " # generate text\n",
        "generated_text = generate_seq(model, tokenizer, seq_length, seed_text, 25)\n",
        "print(\"Generated text: \" + generated_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed text: tapped on the front glass want to get one of those she said earnestly want to get one for the apartment nice to we backed up to a grey old man who bore an absurd resemblance to john d rockefeller in a basket swung from his neck cowered a dozen very\n",
            "\n",
            "Generated text: recent puppies of an indeterminate breed kind are often on the corrugated room of the ventura effort servants an overenlarged motor these cars moved here\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCKrPvslHf2J"
      },
      "source": [
        "## **Sources**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P48IyeEAHkje"
      },
      "source": [
        "https://stackabuse.com/text-generation-with-python-and-tensorflow-keras/\n",
        "\n",
        "https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/\n",
        "\n",
        "https://www.thepythoncode.com/article/text-generation-keras-python\n",
        "\n",
        "http://ethen8181.github.io/machine-learning/keras/rnn_language_model_basic_keras.html"
      ]
    }
  ]
}